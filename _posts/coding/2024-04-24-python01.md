---
layout: post
title: python을 이용한 크롤링 01
date: 2024-04-24 19:20 +0900
description: 
image: ../assets/img/python.jpg
category: coding
tags: python 크롤링 데이터 가져오기
published: true
sitemap: true
---
안녕하세요. 오늘은 제가 크롤링을 공부하면서 정리해가는 python 글입니다.

먼저 전체 코드를 보여드리겠습니다.
```python
import requests as req
from bs4 import BeautifulSoup as bs
import pandas as pd
import datetime  # datetime 모듈 추가

head = {'user-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15"}

res = req.get("https://music.bugs.co.kr/chart", headers=head)

soup = bs(res.text, "lxml")

# 데이터 선택
ranking = soup.select(".ranking > strong")
title = soup.select(".title > a")
artist = soup.select(".artist > a:nth-child(1)")

# 데이터 저장
rankingList = [r.text.strip() for r in ranking]
titleList = [t.text.strip() for t in title]
artistList = [a.text.strip() for a in artist]

# 데이터 프레임 생성
chart_df = pd.DataFrame({
    'Ranking': rankingList,
    'Title': titleList,
    'Artist': artistList
})

# 현재 날짜를 YYYYMMDD 형식으로 가져오기
current_date = datetime.date.today().strftime("%Y%m%d")

# JSON 파일로 저장 (파일 이름에 날짜 추가)
file_name = f"bugsChart100_{current_date}.json"
chart_df.to_json(file_name, force_ascii=False, orient="records")
```
저는 먼저 벅스 뮤직의 랭킹 차트를 가져오기 위해 코드를 작성했습니다.

가져오는 방법은 사이트마다 구조가 달라 조금씩 변경이 있겠지만 큰 틀은 같습니다.

아래에 하나하나 뜯어가며 설명해보도록 하겠습니다.

```python
import requests as req
from bs4 import BeautifulSoup as bs
import pandas as pd
import datetime
```

여기서는 필요한 라이브러리를 import 합니다. 
- `requests`는 웹페이지에 HTTP 요청을 보내기 위해 사용됩니다.
- `BeautifulSoup`은 HTML을 파싱하는 데 사용되며, 웹페이지의 특정 요소를 추출하는 데 도움이 됩니다.
- `pandas`는 데이터를 다루는 데 사용되며, DataFrame을 생성하고 데이터를 JSON 파일로 저장하는 데 사용됩니다.
- `datetime`은 날짜 및 시간 정보를 다루는 데 사용됩니다.

```python
head = {'user-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15"}
```

여기서는 HTTP 요청 시 사용할 user-Agent를 지정합니다. 이 부분은 웹 서버로부터 가져오는 데이터의 형식을 지정하는데 사용됩니다. 만약 웹 서버에서 비정상적인 접속 (크롤링)을 막아 놓았을 시를 대비하여 사용합니다.

```python
res = req.get("https://music.bugs.co.kr/chart", headers=head)
```

`requests` 라이브러리의 `get` 함수를 사용하여 주어진 URL에서 웹페이지의 HTML을 가져옵니다. 이때 `headers` 매개변수를 사용하여 user-Agent를 지정합니다. 유저정보를 웹 서버에 전송함으로써 정상적인 접속으로 인식하게끔 합니다.

```python
soup = bs(res.text, "lxml")
```

`BeautifulSoup` 객체를 생성하여 웹페이지의 HTML을 파싱합니다. 여기서는 `lxml` 파서를 사용합니다.

`lxml`은 파이썬에서 XML 및 HTML을 파싱하기 위한 파서 중 하나입니다. 

일반적으로 `lxml`은 파이썬에서 빠르고 유연하며 효율적으로 XML 및 HTML을 처리하는 데 사용됩니다. 이를 통해 웹 스크레이핑 및 데이터 추출 작업에 적합합니다.

`lxml`을 사용하면 XPath나 CSS 선택자를 이용하여 문서에서 요소를 선택할 수 있습니다. 이는 웹페이지의 특정 요소를 쉽게 찾아내는 데 도움이 됩니다.

일반적으로 `lxml`은 파이썬의 `ElementTree` API를 따르며, XML 및 HTML 문서를 파싱하고 구문 분석하는 데 매우 효율적입니다.

따라서 `lxml`은 웹 스크레이핑 및 데이터 추출 작업을 위한 파이썬의 강력한 도구 중 하나로 널리 사용됩니다.

```python
ranking = soup.select(".ranking > strong")
title = soup.select(".title > a")
artist = soup.select(".artist > a:nth-child(1)")
```

`select` 메서드를 사용하여 웹페이지에서 필요한 데이터를 선택합니다. 여기서는 순위, 제목, 아티스트명을 선택합니다. CSS 선택자를 사용하여 원하는 요소를 선택합니다.
여기서 사이트 구조에 따라서 선택자를 변경해주어야 합니다.

```python
rankingList = [r.text.strip() for r in ranking]
titleList = [t.text.strip() for t in title]
artistList = [a.text.strip() for a in artist]
```

선택한 데이터를 리스트에 저장합니다. `text` 속성을 사용하여 요소의 텍스트를 가져오고, `strip()` 메서드를 사용하여 좌우 공백을 제거합니다.
제가 필요한 데이터는 텍스트 값이기 때문에 텍스트 값 이외에 나머지는 받아오지 않습니다.

```python
chart_df = pd.DataFrame({
    'Ranking': rankingList,
    'Title': titleList,
    'Artist': artistList
})
```

`pandas`의 `DataFrame`을 사용하여 데이터를 테이블 형식으로 정리합니다.
이를 통하여 긁어 온 데이터를 사용하기 좋은 구조로 바꿔줍니다.

```python
current_date = datetime.date.today().strftime("%Y%m%d")
```

현재 날짜를 `datetime` 모듈을 사용하여 YYYYMMDD 형식으로 가져옵니다.
이는 매일 랭킹 차트가 변경되고 가 날짜의 차트라고 표현하기 위해 작성하였습니다.

```python
file_name = f"bugsChart100_{current_date}.json"
```

파일 이름을 현재 날짜를 포함하여 지정합니다.

```python
chart_df.to_json(file_name, force_ascii=False, orient="records")
```

DataFrame을 JSON 파일로 변환하여 저장합니다. `force_ascii=False`는 ASCII 이외의 문자를 그대로 유지하도록 지정하고, `orient="records"`는 JSON 파일을 레코드 단위로 저장하도록 지정합니다.
이를 통해 뽑아낸 데이터를 날짜별로 저장할 수 있게 됐습니다.

이렇게 코드를 하나씩 설명해 보았습니다.

오늘도 필자의 부족한 글 읽어주셔서 감사합니다.